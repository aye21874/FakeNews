# Data transformation

```{r show_col_types = FALSE}
library(tidyverse)
library(patchwork)
library(ggplot2)
library(mi)
library(extracat)
library(dplyr)
library(tidyverse)
library(patchwork)
library(ggplot2)
library(mi)
library(extracat)
library(dplyr)
library(plyr)
library(vcd)
library(grid)
library(tidyverse)
library(gridExtra)
library(scales)
library(openintro)
library(GGally)
library(ggplot2)
library(parcoords)
library(Lock5withR)
library(htmltools)
library(d3r)
library(ggalluvial)
library(wordcloud)
library(tm)
library(tidytext)
library(viridis)
library(sjmisc)
```

```{r}
df_test = read_tsv(file = 'test.tsv' )
t = c("ID","Label","Statement","Subject(s)","Speaker","Speaker Job Title","State Info","The Party Affiliation","Barely True Counts","False Counts","Half True Counts","Mostly True Counts","Pants on Fire Counts","Venue/Location")
colnames(df_test)<- as.vector(t)

df_train = read_tsv(file = 'train.tsv' )
t = c("ID","Label","Statement","Subject(s)","Speaker","Speaker Job Title","State Info","The Party Affiliation","Barely True Counts","False Counts","Half True Counts","Mostly True Counts","Pants on Fire Counts","Venue/Location")
colnames(df_train)<- as.vector(t)


df_train = read_tsv(file = 'train.tsv' )
t = c("ID","Label","Statement","Subject(s)","Speaker","Speaker Job Title","State Info","The Party Affiliation","Barely True Counts","False Counts","Half True Counts","Mostly True Counts","Pants on Fire Counts","Venue/Location")
colnames(df_train)<- as.vector(t)

df_validate = read_tsv(file = 'valid.tsv' )
t = c("ID","Label","Statement","Subject(s)","Speaker","Speaker Job Title","State Info","The Party Affiliation","Barely True Counts","False Counts","Half True Counts","Mostly True Counts","Pants on Fire Counts","Venue/Location")
colnames(df_validate)<- as.vector(t)

fakeNews_df <- rbind(df_train,df_test,df_validate)
fakeNews_df$ID <- seq.int(nrow(fakeNews_df))
```

```{r}
colSums(is.na(fakeNews_df)) %>%
  sort(decreasing = TRUE)

#Finding Rows which have Subjects, Speaker ... Pants on Fire Counts all zero
missing = fakeNews_df[is.na(fakeNews_df$Speaker), ]  

fakeNews_df_transformed = fakeNews_df %>% 
  filter(!ID == 5872 & !ID == 8180)

cat("\n")
print("Removing ID's 5872 and 8180 which had missing values for almost all columns")
print("Replacing all NA's in Subject, Speaker job Title, State Info, Venue/Location, Speaker, The Party Affiliation with UNKNOWN")
cat("\n")
cat("\n")

fakeNews_df_transformed[is.na(fakeNews_df_transformed)] = "unknown"

colSums(is.na(fakeNews_df_transformed)) %>%
  sort(decreasing = TRUE)

```



```{r}

#https://www.johnabernau.com/code/text_analysis/

cleaningFakeNewsColumn <- function(inputColumn, nrows ) { 
  for (i in 1:nrows)
  {
    moby <- tolower(inputColumn[i]) # Lowercase
    #print(moby)
    moby <- gsub("[[:punct:]]", "", moby) # Remove punctuation
    moby <- gsub("[[:digit:]]", "", moby) # Remove numbers
    moby <- gsub("\\s+", " ", str_trim(moby)) # Remove extra whitespaces
    vocab <- unlist(str_split(moby, " ")) # Split into vocab list
    total_words <- length(vocab) # Total words
    unique_words <- length(unique(vocab)) # Unique Words
    lex_div <- unique_words / total_words # Lexical Diversity 
    vocab_nsw <- vocab[!(vocab) %in% stop_words$word] # Only keep non-stopwords
    moby_clean <- paste(vocab_nsw, collapse = " ") # Gather back into one string
    inputColumn[i] = moby_clean
    #print(moby_clean) # Examining our cleaned text block
  }
  return(inputColumn)
}

cleanColumn = cleaningFakeNewsColumn(fakeNews_df_transformed$`Venue/Location`, nrow(fakeNews_df_transformed))
fakeNews_df_transformed["Venue"]= cleanColumn

```

```{r}
# It takes approximately 10min
newstring <- group_str(fakeNews_df_transformed$Venue, precision = 3, strict = TRUE)
print(length(newstring))

```


```{r}
newstring[11375]="unknown"
newstring[11376]="unknown"
fakeNews_df_transformed["groupedVenue"] = newstring
```


```{r}
write.csv(fakeNews_df_transformed,"FakeNewsUpdated.csv", row.names = FALSE)
```


