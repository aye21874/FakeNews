# Results

<!-- Summary

<!-- 1) For this section, we have tried answering 10 questions below, which we find  intriguing from the dataset.  -->

<!-- 2) For each of them we tried summarizing the steps on how we approached this particular question and what assumptions were taken.  -->

<!-- 3) We have also summarized each graph and in the process answered each relevalant question.  -->

<!-- Note :- We have used the data from "FakeNews_Clean.csv" obtained after cleaning/processing the datasset. From this file, we have created 2 dataframes. One for the Fake News grouped by Labels ("false","half-true","pants-fire","barely-true") and the other by Factual News grouped by Label ("True", "stly-True").  -->


```{r show_col_types = FALSE, warning=FALSE, error=FALSE}
library(tidyverse)
library(patchwork)
library(ggplot2)
library(mi)
library(extracat)
library(dplyr)
library(plyr)
library(vcd)
library(grid)
library(tidyverse)
library(gridExtra)
library(scales)
library(openintro)
library(GGally)
library(ggplot2)
library(parcoords)
library(Lock5withR)
library(htmltools)
library(d3r)
library(ggalluvial)
library(wordcloud)
library(tm)
library(tidytext)
library(ggridges)
library(geofacet)
library(viridis)
library(usmap) #import the package
```


```{r}
theme_fakeNews_rotate <- function(base_size = 14, ang=90) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 25, face = "bold", margin = margin(10,10,10,10), hjust = 0.5),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.x = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.y = element_text(size = 13, face="bold", margin = margin(10,10,10,10)),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 13, face = "bold"),
      legend.text = element_text(size = 13, face = "bold")
    )}

theme_fakeNews <- function(base_size = 14) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 25, face = "bold", margin = margin(10,10,10,10), hjust = 0.5),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.x = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.y = element_text(size = 13, face="bold", margin = margin(10,10,10,10)),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 13, face = "bold"),
      legend.text = element_text(size = 13, face = "bold")
    )}

#Reference : https://benjaminlouis-stat.fr/en/blog/2020-05-21-astuces-ggplot-rmarkdown/
#Reference for legend size: https://www.statology.org/ggplot2-legend-size/

theme_ben <- function(base_size = 14, ang=90) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 13, face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 13, face = "bold"),
      axis.text.x = element_text(angle = ang, size = 13, face = "bold"),
      axis.text.y = element_text(size = 13),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 13, face = "bold"),
      legend.text = element_text(size = 13, face = "bold")
    )}

theme_ben2 <- function(base_size = 10, ang = 90, add=0) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 9+add, face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 9+add, face = "bold"),
      axis.text.x = element_text(angle = ang, size = 8, face = "bold"),
      axis.text.y = element_text(size = 9+add),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 8+add, face = "bold"),
      legend.text = element_text(size = 8+add, face = "bold"),
      legend.key.size = unit(0.4, 'cm')
    )}

```


```{r}

# Reading from the file
# Filtering based on the Label.
# Created 2 dataframes,
    # One for fake news (grouped by Label["false","half-true","pants-fire","barely-true"])
    # and other one for Factual News (grouped by Label["True", "Mostly-True"])

fakeNews_df_transformed = read_csv(file = 'FakeNews_Clean.csv' )
fakeNews_df_transformed_false = filter(fakeNews_df_transformed, Label=="false"| Label =="barely-true" | Label=="half-true" | Label=="pants-fire")
fakeNews_df_transformed_true = filter(fakeNews_df_transformed, Label=="true"| Label =="mostly-true")
```


**Question 1: Which positions of authority lead to Fake & Factual information? Which party tends to spread Fake News more? **

```{r fig.height=12, fig.width=20}
df_1 = fakeNews_df_transformed %>%
        group_by(fakeNews_df_transformed$SpeakerJobTitle) %>%
        dplyr::summarise(n = n()) %>%
        arrange(desc(n)) %>%
        filter(n>=100)


t = c("SpeakerJobTitle","Count")
colnames(df_1)<- as.vector(t)

new_vector <- df_1$SpeakerJobTitle


df_3 <- fakeNews_df_transformed[fakeNews_df_transformed$SpeakerJobTitle %in% new_vector,]

df_4 <- df_3 %>%
        filter(df_3$SpeakerJobTitle != "unknown")


#VVIMPT
df_4$SpeakerJobTitle = tolower(df_4$SpeakerJobTitle)


df_5 = df_4 %>%
        group_by(df_4$SpeakerJobTitle, df_4$Label) %>%
        dplyr::summarise(n = n())


t = c("SpeakerJobTitle","Label","Count")
colnames(df_5)<- as.vector(t)

df_5$SpeakerJobTitle=  df_5$SpeakerJobTitle %>% recode(
                      "attorney" = "Attorney",
                      "congress" = "Congress",
                      "president" = "President",
                      "presidentelect" = "President Elect",
                      "governor" = "Governor",
                      "presidential candidate" = "President Candidate",
                      "house representative, house representatives" = "House_Official",
                      "milwaukee county executive" = "County Executive",
                      "senator, senators" = "Sentators",
                      "congress, congressman, congressman ny, congresswoman" = "Congress",
                      "representative, representativej" = "Representative")
                     
         
#Do high positions of authority lead to misleading information - based on the distribution of data in dataset ?

ggplot(df_5, aes(x=df_5$SpeakerJobTitle, y=df_5$Count))+
  geom_bar(stat='identity', fill="forest green")+
  ylab("Count for Fake & Factual News") +  xlab("Speaker Job Title") +
  facet_wrap(~df_5$Label,  ncol=1) +
  theme_fakeNews_rotate()
```

**Observation from Graph 1.1**

1) President, Governor and Senators spread the most news in all categories of truthfulness.

2) The "President Elect" & "President Candidate" had very similar news counts throughout the truthfulness spectrum

3) Senators from the Party lead the count for spreading information, for both Factual & Fake. The graph also demonstrates highest pants-fire count for Senators, demonstrating that they spread the most ridiculous claims out of all job titles in the graph.

4) For all job titles, the counts for mostly-true news were higher than true news.

5) House Officials have had the least counts in spreading Fake or Factual News.

6) The total count for pants-fire, false, barely-true, and half-true news is higher than the total count for mostly-true and true news, signifying that almost all job titles in the graph are associated with a  high proportion of Fake News.



```{r fig.height=12, fig.width=20}

df_8 = df_4 %>%
        group_by(df_4$SpeakerJobTitle, df_4$PartyAffiliation, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n))

t = c("SpeakerJobTitle","ThePartyAffiliation", "Label","Count")
colnames(df_8)<- as.vector(t)

df_9 = filter(df_8, Label == "false" | Label == "true")

df_10 = filter(df_9, ThePartyAffiliation=="independent" | ThePartyAffiliation=="republican" | ThePartyAffiliation=="democrat")

df_10$SpeakerJobTitle=  df_10$SpeakerJobTitle %>% recode(
                      "attorney" = "Attorney",
                      "congress" = "Congress",
                      "president" = "President",
                      "presidentelect" = "President Elect",
                      "governor" = "Governor",
                      "presidential candidate" = "President Candidate",
                      "house representative, house representatives" = "House_Official",
                      "milwaukee county executive" = "County Executive",
                      "senator, senators" = "Sentators",
                      "congress, congressman, congressman ny, congresswoman" = "Congress",
                      "representative, representativej" = "Representative")


knitr::opts_chunk$set(
 fig.width = 6,
 fig.asp = 0.8,
 out.width = "80%"
)
# Stacked
ggplot(df_10, aes(fill=Label, y=Count, x=SpeakerJobTitle)) +
  geom_bar(position="stack", stat="identity") +
  xlab("Speaker Job Title") +
  ylab("Count for Fake News") +
  labs(fill='Label') +
  facet_wrap(~ThePartyAffiliation,  ncol=1) + theme_fakeNews()

```

**Observation from Graph 1.2**

1) The proportion of true news is higher than the proportion of false news for the Democrat and Independent parties, with the opposite case for the Republican party. This tells us that the Democrat and Independent parties are a little bit more reliable in delivery of truthful news.
2) There is false news present for most speakers in every party, albeit at different proportions. 


**Question 2: What are the most selected topic/Subject picked by each Speaker which has the highest tendency of Spreading Fake News"**

```{r fig.width=17, fig.height=10}

# Most selected topic by each authority power, and relationship with fake news

df_6 = df_4 %>%
        group_by(df_4$SpeakerJobTitle, df_4$Subject, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n)) %>%
        filter(n>=4)

t = c("SpeakerJobTitle","Subjects", "Label","Count")
colnames(df_6)<- as.vector(t)

df_12 = filter(df_6, Label == "false" | Label == "half-true")


df_12$Subjects=  df_12$Subjects %>% recode(
                     "education"="Education",
                     "healthcare"="HealthCare",
                     "elections"="Elections",
                     "immigration"="Immigrations",
                     "economyjobs"="EconomyJobs",
                     "federalbudget"="Fedral Budget",
                     "labor, taxes, water, weather, women" = "Labor, Weather, Taxes",
                     "abortion"="Abortion",
                     "debt, food, guns, jobs, polls" = "Debts, Food, Jobs",
                     "fires, iraq, islam, israel, trade, urban" = "Fires, Iraq, Israel, Islam",
                     "energy"="Energy",
                     "candidatesbiography"="Candidates Biography",
                     "economy"="Economy",
                     "foreignpolicy"= "Foreign Policy",
                     "military" = "Military")

df_12$SpeakerJobTitle = df_12$SpeakerJobTitle %>% recode(
                      "attorney" = "Attorney",
                      "congress" = "Congress",
                      "president" = "President",
                      "presidentelect" = "President Elect",
                      "governor" = "Governor",
                      "presidential candidate" = "President Candidate",
                      "house representative, house representatives" = "House_Official",
                      "milwaukee county executive" = "County Executive",
                      "senator, senators" = "Sentators",
                      "congress, congressman, congressman ny, congresswoman" = "Congress",
                      "representative, representativej" = "Representative")


ggplot(df_12, aes(fill=df_12$SpeakerJobTitle, y=df_12$Count, x=df_12$Subjects)) +
  geom_bar(position="dodge", stat="identity") +
  xlab("X Asix") +
  ylab("Y Axis") +
  labs(fill = "Speaker Job Title") + scale_fill_viridis(discrete=TRUE) +
  theme_fakeNews() + coord_polar()


```


**Observation from Graph**

1) Senators have spread the most Fake News with respect to every topic present in the graph

2) President Candidates have spread fake news only in Immigration

3) President didn't report Fake News related to topics in Military, Abortion, Debts, Jobs, and Economy.

4) Only Senators and President had fake news related to topics in Fires, Iraq, Israel, Islam, Foreign Policy, Fedral Budge, and Energy.

5) Education has the highest count for fake news.



**Question 3: Which states are spreading the most false news, also noting the number of different speakers involved in spreading Fake News?**

```{r fig.width=13, fig.height=8}

df_80 = fakeNews_df_transformed_false %>%
      group_by(fakeNews_df_transformed_false$StateInfo) %>%
      dplyr::summarise(n = n()) %>%
      arrange(desc(n))

t = c("State","NumberofFalseCounts")
colnames(df_80)<- as.vector(t)

df_81 = fakeNews_df_transformed_false %>%
        group_by(fakeNews_df_transformed_false$StateInfo, fakeNews_df_transformed_false$Speaker) %>%
        dplyr::summarise(n = n()) %>%
        arrange(desc(n))

t = c("State"," UniqueSpeakerPerState","Count")
colnames(df_81)<- as.vector(t)

df_82 = df_81 %>%
      group_by(df_81$State) %>%
      dplyr::summarise(n = n()) %>%
      arrange(desc(n))


t = c("State","NumberofSpeakers")
colnames(df_82)<- as.vector(t)

df_80["NumberofSpeakers"] =df_82$`NumberofSpeakers`
df_83 = df_80 %>% filter(NumberofFalseCounts>20)
```


```{r fig.width=13, fig.height=10, echo=FALSE, message=FALSE, results='hide'}

treemap::treemap(df_83,
       index=c("State"),
       vSize="NumberofFalseCounts",
       vColor="NumberofSpeakers",
       type="value",
       format.legend = list(scientific = FALSE, big.mark = " ")) + theme_fakeNews()
```

**Observation from the Graphs**

Above Graph gives a clear idea for the count of Fake News in each State. The larger the rectangle, the higher the number of Fake News was spread within the associated state. The color is mapped with the different number of speakers who spread fake news in the given state.

1) There were lot of states in the dataset which were labelled Unknown while pre-processing, which apparently are also spreading Fake News in high proportions

2) Texas is most widely known state to spread Fake News with about 400+ different Speakers involved in spreading Fake News

3) Count of Fake News spread is closely related with the different number of Speakers spreading the news. States with a smaller number of false counts have fewer number of speakers.

4)  New York, Wisconsin and Florida have almost equal number of counts in spreading Fake News, which are comparatively high when compare to other states. Also, Wisconsin and New York share the same count for number of speakers but the Florida has highest number of different speakers among the three.




**Question 4: Which method of communication has resulted in the most Fake News spread by state? **

```{r fig.width=25 , fig.height=10}

# We have used Catograms to answer this question. Below is the step wise procedure for the same.
#
# Step 1) Columns Used Label, Venue, State.
#
# Step 2) Generate the count for Fake News spread using a particular Platform (Venue) in a particular region (StateInfo).
#
# Step 3) Filtered the Unknown States from the dataset, by removing those rows. An important step here will be to make sure regions in both the dataframes are either lower case or upper case so that they can be matched.
#
# Step 4) For each state, obtained the Platform (Venue) with max fake news count. We also avoided the Platforms which had a count of 1 Fake News, which was not enough to validate if Platform is actually spreading fake news.
#
# Step 5)  Using the state dataframe of USA (imported form library), merged our dataframe (using left join), to get longitudes and latitudes of each states, so that they can plotted on the Catogram.
#
# Step 6) To make sure that Venues like "news conference, press conference", which were grouped together, are recoded with proper names, to make sure they have better names on the graph.


df_51 = fakeNews_df_transformed_false %>%
        group_by(fakeNews_df_transformed_false$Venue, fakeNews_df_transformed_false$StateInfo) %>%
        dplyr::summarise(n = n()) %>%
        arrange(desc(n))

t = c("GroupedVenue","region","n")
colnames(df_51)<- as.vector(t)

df_52 = filter(df_51, region != "unknown")

state <- map_data("state")
state$region = tolower(state$region)
df_52$region = tolower(df_52$region)

df_53 = df_52 %>%
            group_by(region) %>%
            filter(n==max(n) & n>=2)

df_54 <- left_join(state, df_53, by = c('region'))

df_54$GroupedVenue=  df_54$GroupedVenue %>% recode(
                      "news conference, press conference" = "Conferences",
                      "television interview" = "Television Interview",
                      "campaign ad, campaign tv ad, campaign web ad" = "Campaign Advertisement",
                      "speech, speeches" = "Speeches",
                      "tv ad, tv ads, tv talk, web ad"="Television Advertisement",
                      "unknown" = "Unknown",
                      "internet, interview, interviews"="Online Interviews",
                      "vice presidential debate, vicepresidential debate" = "Vice-Presidential Debate",
                      "campaign commercial, campaign commerical, campaign tv commercial, tv campaign commercial" = "Commercial Campaign",
                      "senate floor speech" = "Senate Floor Speech",
                      "alert, retweet, ticket, tweet, tweets" = "Tweets",
                      "interview cnn, interview cspan, interview ed, interview union" = "Interview Cnn",
                      "news release"  = "News Release",
                      "opinion colulmn, opinion column" = "Opinion Column",
                      "comments abcs week" = "Comments Week",
                      "date, debate, senate, tv debate"  = "Debate",
                      "public statement, public statements" = "Public Statements",
                      "speech senate floor" = "Senate Speech",
                      "interview fox news sunday" = "Fox News Interview",
                      "press releaase, press release, press releases" = "Press Release",
                      )


ggplot(data=df_54, aes(x=long, y=lat, fill=GroupedVenue, group=region)) +
  geom_polygon(color = "white") + ggtitle("Most Popular Platforms to Spread Fake News State Wise") + xlab("Longitude") + ylab("latitude") +
  theme_fakeNews()

```


**Observation from Graph**

Below are a few venues/platforms which were used the most to spread fake news per State.

1) Florida, Texas, California, Indiana, Maryland, Missouri, and North Carolina mostly spread Fake News through Press Releases.

2) Georgia, Illinois, Pennsylvania, Virginia mostly spread Fake News through Speeches.

3) Many states used a variety of venues to spread fake news, rather than just one consistent venue. Hence the gray spots in the graph.

4) Our grouping of the Venues (such as Tweets/Tweet to Tweet) might not be completely accurate since we used group_str to do the concatenation with the distance of similarity as 3 between words. This parameter can be manipulated for better efficiency.



**Question 5: Correlations, Clusters & Outliers observed for different Venues/Platforms in spreading Fake News for each State. **


```{r fig.width=17, fig.height=10 }

# Step 1) Columns Used are :- Label, Venue (Platforms)
#
# Step 2) Using the fake news dataframe, generate the count for Fake News spread using a particular Platform (Venue)
#
# Step 3) Pivot Wider the dataset to create columns for each unique value present in the Venue. Now, we will have a dataframe with each Region covering the count for the times the particular Platform/Venue was used to spread Fake News.
#
# Step 4) Since, not all platforms/Venue were not used for each state, we selected a particular portion of dataframe (13 states and 10 Platforms/Venue) which had the least null values.
#
# Step 5) Then we imputed the null values with the mean of each row i.e. mean of platforms used for that region
#
# Step 6) Recoded the column names for better understanding in graph and plotted them using the ggparcoord


df_60 = df_52 %>%
            group_by(GroupedVenue)

df_61 = df_60 %>%
  pivot_wider(names_from = GroupedVenue, values_from = n)

#snapshot with almost no missing values
df_62 = df_61[1:13, 1:10]

df_63 = df_62 %>% replace(is.na(.), 0) %>% mutate(row_wise_mean = rowMeans(.[2:10]))

for (row in 1:nrow(df_63)) {
  for(col in 1:(ncol(df_63)-1)){
    if(df_63[row,col]==0)
    df_63[row,col] = as.integer(df_63[row,"row_wise_mean"])
  }
}
t = c("Region","Online Interview","Press Release","News Release","TV Advertisement","Speeches","News Conferences","Radio Interview","Tweets", "TV Interview","Row_Wise_Mean")
colnames(df_63)<- as.vector(t)

ggparcoord(df_63, columns =2:10, scale = "globalminmax", groupColumn = 1)  + theme_fakeNews() + xlab("Venue used to spread Fake News") + ylab("Frequency of Fake News")


# df_63[,1:10]  %>%
#   parcoords(
#     rownames = F
#     , brushMode = "1D-axes"
#     , reorderable = T
#     , queue = T
#     , color = list(
#     colorScale = "scaleOrdinal",
#     colorBy = "Region",
#     colorScheme = "schemeCategory10"
#   ),
#   ,withD3 = TRUE
#   ,height = 700
#   ,width = 1000
#   )

```


**Observation from the Graphs**

1) The count of Fake News spread for the venue OnlineInterview was less than 10 for all regions except Wisconsin, Texas and Florida.

2) Most of the States, which have used Press Release for spreading Fake News, have also used News Release. We observed a positive correlation between these two venues for most States

3) Similarly, we noticed a negative correlation between News Release and TV Advertisement. The states which have used News Release have a reduced count for TV Advertisements

4) We noticed a wide range of differences in count between New York & Jersey for each venue.

5) Surprisingly, Twitter had relatively low frequencies for each State despite being the most popular social media platform.



**Question 6: Count of Words in a Statement in relation with its Label**


```{r fig.width=12, fig.height=10}

wordsCount <- vector()
LabelData <- vector()
SpeakerJobTitle <- vector()


for(i in 1:nrow(fakeNews_df_transformed)) {
  split <- strsplit(fakeNews_df_transformed$Statement[i], " ")
  t = sapply( split , length)
  wordsCount[i] = t
  LabelData[i] = fakeNews_df_transformed$Label[i]
  SpeakerJobTitle[i] = fakeNews_df_transformed$SpeakerJobTitle[i]
}



df_21 = data.frame(wordsCount,LabelData,SpeakerJobTitle)
df_22 = aggregate(df_21$wordsCount, by=list(df_21$SpeakerJobTitle,df_21$LabelData), FUN=sum)
df_23 = filter(df_22, x>=100 & x<=1000)

df_24 = df_23 %>%
      mutate(newLabel = ifelse((Group.2=="false" | Group.2 =="barely-true" | Group.2=="half-true" | Group.2=="pants-fire"), "false","true"))


df_24$Group.1=  df_24$Group.1 %>% recode(
                "actor, auditor, author, mother" = "Actor/Author",
                "advocacy" = "Advocacy",
                "attorney"  = "Attorney",
                "banker, farmer, laws, lawyer, mayor" = "Banker/Lawyer",
                "businessman, businesswoman"  = "Business Individual",
                "candidate senate physician" = "Physician Senate",
                "cohost cnns crossfire"  = "Cohost Cnns",
                "congress, congressman, congressman ny, congresswoman" = "Congress Official",
                "consultant" = "Consultant",
                "governor jersey" = "Governor Jersey",
                "host, house" = "House Host",
                "house district" = "House District",
                "house majority leader, house minority leader" = "House Majority Leader",
                "house representative, house representatives" = "House Official",
                "lieutenant governor"  = "Lieutenant_Governor",
                "mayor austin, mayor houston"= "Mayor",
                "milwaukee county executive" = "County Executive",
                "ohio governor" = "Ohio Governor",
                "political action committee, politican action committee" = "Action Committee",
                "presidentelect" = "President Elect",
                "presidential candidate" = "Presidential Candidate",
                "radio host, radiotv host" = "Radio Host",
                "representative florida district" = "Florida Official",
                "senate majority leader, senate minority leader" = "Senate Majority",
                "senator district, senator st district"  = "Senator District",
                "senator ohio" = "Senator Ohio",
                "social media posting" = "Social Media Posting" ,
                "speaker house representatives, speaker nc house representatives" = "Speaker House",
                "texas congressman houst representatives" = "Texas Congress",
                "chairman republican national committee, cochairman republican national committee" = "Chairman committee",
                "columnist" = "Columnist",
                "governor ohio jan" = "Governor Ohio",
                "mayor milwaukee" = "Mayor Milwaukee",
                "mayor providence" = "Mayor Providence",
                "ohio treasurer, ri treasurer, treasurer" = "Treasurer",
                "political commentator" = "Political Commentator",
                "venture capital company founder" = "Vebture Capital",
                "assembly district"  = "Assembly District",
                "assemblyman, assemblywoman"  = "AssemblyMan/Woman",
                "delegate" = "Delegate",
                "madison school board" = "Madison School",
                "philanthropist" = "Philanthropist",
                "lieutenant governorelect" = "Lieutenant Elect Governor",
                "president heritage foundation" = "President Heritage",
                "secretary"  = "Secretary",
                "president" = "President",
                "representative, representativej" = "Party Representative ",
                "agriculture commissioner" = "Agriculture Commissioner",
                 "msnbc host" = "Msnbc Host",
                 "texas house representative" = "Texas House Official")
                                                                 

t = c("SpeakerJobTitle","Label","Count","News_Label")
colnames(df_24)<- as.vector(t)


ggplot(df_24, aes(x = Count, y = reorder(SpeakerJobTitle, Count))) +
  geom_point(aes(colour = News_Label)) +  
  xlab("Count of Words in Statement") + ylab("Speaker Job Title") + theme_fakeNews()

```

**Observation from the Graphs**

Note: the statements shown on the graph are between 100 and 1000 words.

1) For most speakers, there are more true than false statements given that they have less than 300 words.

2) For statements with over 300 words, there is a higher count of true statements compared to false statements.


```{r}
df_fakeNews = read_csv("FakeNews_andrew.csv")
```


**Question 7: What are the proportions of fake news for each state?**


```{r fig.width=13, fig.height=30}

# Which state makes the most claims?
# Reference for ordering: https://www.statmethods.net/management/sorting.html

filt = c("Virginia director, Coalition to Stop Gun Violence", "Unknown", "unknown", "N/A", "Tex", "Russia", "Qatar", "China", "Virgiia", "PA - Pennsylvania", "None", "United Kingdom", "the United States")
df_f3 <- df_fakeNews %>%
  filter(!(df_fakeNews$`State Info` %in% filt))
#df$`State Info` <- with(df, ifelse(df$`State Info`=='Washington, D.C.', "Washington DC", df$`State Info`))

df_f4 = df_f3 %>%
        group_by(df_f3$`State Info`, df_f3$`Label`) %>%
        dplyr::summarise(n = n())
t = c("StateInfo","Label","Count")
colnames(df_f4)<- as.vector(t)


df_order <- df_f3 %>%
        group_by(df_f3$`State Info`) %>%
        dplyr::summarise(n = n())
colnames(df_order)<- as.vector(c("StateInfo", "n"))
df_order <- df_order[order(-df_order$n),]
df_order$Panel <- c(rep(1:6, each = 9), c(6,6,6))
df_order$Panel <- factor(df_order$Panel, levels = 1:6,
                   labels = c("250+ Tweets", "80-270 tweets", "30-80 tweets",
                              "6-30 tweets", "2-6 tweets", "1 or 2 tweets"))

df_f4$total <- df_order$n[match(df_f4$StateInfo, df_order$StateInfo)]
df_f4$panel <- df_order$Panel[match(df_f4$StateInfo, df_order$StateInfo)]
df_f4 <- df_f4[order(-df_f4$total),]

lvl = c("pants-fire", "false", "barely-true", "half-true", "mostly-true", "true")
df_f4 %>%
ggplot(aes(fill = factor(Label, levels = lvl), y = Count, x = reorder(StateInfo,total) )) +
  geom_bar(position="stack", stat="identity") +
  facet_wrap(~panel, ncol=1, scales="free") +
  xlab("State Info") +
  ylab("Count") +
  labs(fill='Label') +
  coord_flip() +
  theme_ben2(ang=0, add=2)

```

**Observation from the Graphs**

1) All states generally report a variety of news ranging throughout the entire truth spectrum (from true to false).

2) The majority of news reports have some level of truth in them, with the exception of Maine and Minnesota.

3) All states follow roughly the same distribution of fake news (of course, with the exception of states with very few reports).

4) Texas reported the most news while Tennessee, Montana, Rhode Island, and Mississippi have the fewest news reports out of all states.

5) Out of all states with over 100 reports, Vermont was the only state that did not make any pants-on-fire reports (in other words, Vermont was the only state with over 100 reports that did not make any ridiculous claims).



**Question 8: What are the states with maximum proportions of fake news and in what subject?**

```{r fig.width=13}
# Reference: https://stackoverflow.com/questions/32578082/r-how-to-replace-value-of-a-variable-conditionally


# Getting the percentage of lies for states with over 10 statements
dff <- df_f3
df_ord <- dff %>%
        group_by(dff$`State Info`) %>%
        dplyr::summarise(n = n())
colnames(df_ord)<- as.vector(c("StateInfo", "n"))

false = c("barely-true", "false", "pants-fire", "half-true")
true = c("mostly-true", "true")

dff$Label <- with(dff, ifelse(Label %in% false, "not factual", Label))
dff$Label <- with(dff, ifelse(Label %in% true, "factual", Label))
encoded <- dff



# Reference: https://statisticsglobe.com/select-top-n-highest-values-by-group-in-r

txt <- data_frame(Text = encoded$`Subject(s)`, Label=encoded$`Label`, State=encoded$`State Info`) %>%
  unnest_tokens(output = word, input = Text, token = 'regex', pattern=",")
txt_total <- txt %>% group_by(txt$word, txt$State) %>%
  dplyr::summarise(n = n()) %>%
  arrange(desc(n))
txt <- txt %>%
  group_by(txt$word, txt$Label, txt$State) %>%
  dplyr::summarise(n= n()) %>%
  arrange(desc(n))
t1 = c("word", "Label", "state", "n")
t2 = c("word", "state", "total")
colnames(txt) <- as.vector(t1)
colnames(txt_total) <- as.vector(t2)
txt <- merge(txt, txt_total)


prop <- txt %>%
  filter(Label == "not factual") %>%
  filter(total > 2)
prop$perc <- round(100*prop$n/prop$total, digits=1)
prop <- prop %>%
  arrange(desc(perc)) %>%
  group_by(state) %>%
  slice(1:1)


dff = dff %>%
        group_by(dff$`State Info`, dff$`Label`) %>%
        dplyr::summarise(n = n())
colnames(dff) <- as.vector( c("StateInfo","Label","Count") )
dff$total <- df_ord$n[match(dff$StateInfo, df_ord$StateInfo)]
dff <- dff %>%
        filter(Label == "not factual") %>%
        filter(total > 10)
dff$liesPercentage <- round(100*dff$Count/dff$total, digits=1)
dff$word = prop$word[match(dff$StateInfo, prop$state)]


# Reference: https://stackoverflow.com/questions/45437815/geom-text-with-two-labels-in-ggplot-graph

dff %>%
  ggplot(aes(y = liesPercentage, x = reorder(StateInfo, liesPercentage))) +
  geom_col() +
  coord_flip() +
  labs(x = "State \n", y = "\n Percentage of Lies", title = "Percentage of lies by State\n") +
  geom_text(aes(label = paste(liesPercentage,"%, ",word)), hjust = 1.2, colour = "white", fontface = "bold") +
  theme_ben(ang=0)
```

**Observation from the Graphs**
Note: news reports that are labelled with pants-on-fire, false, barely-true, or half-true are considered not factual. Factual news are either mostly-true or true. The legend shows the order of truth values, which we will call the truth spectrum

1) Alabama is the most misinformative state and Conneticut is the most informative.

2) Healthcare is the most misinformative subject out of all subjects in the states, with candidates-biography being the second most misinformative subject.

3) The top 5 most misinformative states are either Southern or Midwestern, while the bottom 3 most misinformative states are located in the Eastern region of the United States

4) Apart from the top 5 and bottom 3 states, there does not seem to be a particular region associated with misinformation spread by state


**Question 9: Distribution of Factual News with respect to state**

```{r}
dfl <- df_f3
dfl$Label <- with(dfl, ifelse(Label=="pants-fire", 0, Label))
dfl$Label <- with(dfl, ifelse(Label=="false", 1, Label))
dfl$Label <- with(dfl, ifelse(Label=="barely-true", 2, Label))
dfl$Label <- with(dfl, ifelse(Label=="half-true", 3, Label))
dfl$Label <- with(dfl, ifelse(Label=="mostly-true", 4, Label))
dfl$Label <- with(dfl, ifelse(Label=="true", 5, Label))
dfl <- transform(dfl, Label = as.numeric(Label))
```

```{r fig.width=12}
ggplot(dfl, aes(x = Label, y = reorder(State.Info, -Label, median))) +
  geom_density_ridges(fill = "blue", alpha = .5) +
  labs(x = "Score", y = "State", title = "Distribution of Misinformation by State") +
  theme_ben2(ang=0)
```
**Observation from the Graphs**
Note: Each label was assigned a score: 0 for "pants-on-fire", 1 for "false", 2 for "barely-true", 3 for "half-true", 4 for "mostly-true", and 5 for "true"

1) Most states either have a very flat, bimodal, or multimodal distribution. This indicates that most states have news reports that range through multiple points in the truth spectrum, which is consistent with the graph in question 7.

2) The top 2 most honest states by median (Wyoming and North Dakota in this case) have most of their distribution around the score of 5.

3) North Dakota is consistently true with their news, while Washington is consistently half-true with their news



**Question 10: Which subject has the most concentrated fake news?**

```{r fig.width=13}
txt <- data_frame(Text = encoded$`Subject(s)`, Label=encoded$`Label`) %>%
  unnest_tokens(output = word, input = Text, token = 'regex', pattern=",")
txt_total <- txt %>% group_by(txt$word) %>%
  dplyr::summarise(n = n()) %>%
  arrange(desc(n))
txt <- txt %>%
  group_by(txt$word, txt$Label) %>%
  dplyr::summarise(n= n()) %>%
  arrange(desc(n))
t1 = c("word", "Label", "n")
t2 = c("word", "n")
colnames(txt) <- as.vector(t1)
colnames(txt_total) <- as.vector(t2)
txt$total <- txt_total$n[match(txt$word, txt_total$word)]
txt <- txt %>%
  arrange(desc(total))


prop <- txt %>%
  filter(Label == "not factual") %>%
  filter(n > 50)

prop$liesPercentage <- round(100*prop$n/prop$total, digits=1)
prop <- prop %>%
  arrange(-liesPercentage) %>%
  head(50)
prop$Panel <- rep(1:2, each = 25)
prop$Panel <- factor(prop$Panel, levels = 1:2,
                   labels = c("Top 25 Controversial Subjects", "Top 25-50"))

prop %>%
  ggplot(aes(y = liesPercentage, x = reorder(word, liesPercentage))) +
  geom_col() +
  coord_flip() +
  labs(x = "Subject \n", y = "\n Percentage Fake News", title = "Percentage of lies by Subject\n") +
  geom_text(aes(label = liesPercentage), hjust = 1.2, colour = "white", fontface = "bold") +
  facet_wrap(~Panel, ncol=1, scales="free") +
  theme_ben(ang=0)

```
**Observation from the Graphs**
Note: a news report is considered fake news if it is either pants-on-fire, false, barely-true, or half-true

1) Religion has the most misinformation spread about it

2) Most of the controversial subjects present in question 8 are also present in this graph (and are therefor the top 50 most misinformative subjects)

3) Like one would expect, most of the subjects present in this graph are related to social issues or political topics.



**Question 11: Origin of Fake Vs Factual News with respect to Venue, Party & Job Title**


```{r fig.width=8}
# Reference: https://r-dir.com/blog/2015/01/quickly-categorize-messy-data.html
# Reference: https://stackoverflow.com/questions/20950221/extracting-first-value-from-a-list

df = read_csv(file = 'FakeNews_Clean.csv' )
df <- df %>%
  group_by(PartyAffiliation) %>%
  filter(n()>300) %>%
  group_by(SpeakerJobTitle) %>%
  filter(n()>200) %>%
  group_by(Venue) %>%
  filter(n()>100)

false = c("barely-true", "false", "pants-fire", "half-true")
true = c("true", "mostly-true")
df$Label <- with(df, ifelse(Label %in% false, "Not entirely factual", Label))
df$Label <- with(df, ifelse(Label %in% true, "Factual", Label))

df <- df %>%
  group_by(Label, PartyAffiliation, SpeakerJobTitle, Venue) %>%
  dplyr::summarise(n = n()) %>%
  arrange(desc(Label))
df[is.na(df)] <- "none"


df$Venue <- sapply(strsplit(df$Venue, ","), function(x) x[1])
df$SpeakerJobTitle <- sapply(strsplit(df$SpeakerJobTitle, ","), function(x) x[1])


df %>%
  ggplot(aes(y=n, axis1=Venue, axis3=Label, axis2=PartyAffiliation, axis4=SpeakerJobTitle)) +
  geom_alluvium(aes(fill = Label)) +
  geom_stratum(fill='black', color='white') +
  geom_label(stat = "stratum", size = 3, aes(label = after_stat(stratum))) +
  scale_x_discrete(limits = c("Venue", "Party", "Label", "Job Title"), expand = c(.1, .1)) +
  ggtitle("Truthfulness of News based on Origin") +
  xlab("Origin of News") +
  ylab("Frequency") +
  scale_fill_brewer(type = "qual", palette = "Set1", direction=-1) +
  theme_ben2(ang=0)
```
**Observation from the Graphs**

1) Fake news is present no matter what venue, party, or type of speaker it originates from

2) For every type of venue, party, and job title, at least half of the news reports are not entirely true

3) As seen in the Label column, most news reports made in this graph are not entirely true

