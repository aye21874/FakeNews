# Results

```{r show_col_types = FALSE}
library(tidyverse)
library(patchwork)
library(ggplot2)
library(mi)
library(extracat)
library(dplyr)
library(plyr)
library(vcd)
library(grid)
library(tidyverse)
library(gridExtra)
library(scales)
library(openintro)
library(GGally)
library(ggplot2)
library(parcoords)
library(Lock5withR)
library(htmltools)
library(d3r)
library(ggalluvial)
library(wordcloud)
library(tm)
library(tidytext)
library(ggridges)
library(geofacet)
library(viridis)
library(usmap) #import the package
```



```{r}
#Reference : https://benjaminlouis-stat.fr/en/blog/2020-05-21-astuces-ggplot-rmarkdown/
#Reference for legend size: https://www.statology.org/ggplot2-legend-size/ 

theme_ben <- function(base_size = 14, ang=90) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 13, face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 13, face = "bold"),
      axis.text.x = element_text(angle = ang, size = 13, face = "bold"),
      axis.text.y = element_text(size = 13),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 13, face = "bold"),
      legend.text = element_text(size = 13, face = "bold")
    )}

theme_ben2 <- function(base_size = 10, ang = 90, add=0) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 9+add, face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 9+add, face = "bold"),
      axis.text.x = element_text(angle = ang, size = 8, face = "bold"),
      axis.text.y = element_text(size = 9+add),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 8+add, face = "bold"),
      legend.text = element_text(size = 8+add, face = "bold"),
      legend.key.size = unit(0.4, 'cm')
    )}

```


```{r}
df_fakeNews = read_csv(file = 'FakeNews.csv' )
```


```{r }
#Do high positions of authority lead to misleading information - based on the distribution of data in dataset ?

# Step 1 : Checking the "Speaker Job Title" which have made large number of statements in the dataset 
           #and thresholding with atleast 40 statements

df_1 = df_fakeNews %>% 
        group_by(df_fakeNews$`Speaker Job Title`) %>% 
        dplyr::summarise(n = n()) %>%
        arrange(desc(n)) %>%
        filter(n>=30) 

t = c("SpeakerJobTitle","Count")
colnames(df_1)<- as.vector(t)

new_vector <- df_1$SpeakerJobTitle

df_3 <- df_fakeNews[df_fakeNews$`Speaker Job Title` %in% new_vector,]

df_4 <- df_3 %>%
        filter(df_3$`Speaker Job Title` != "unknown")

#VVIMPT
df_4$`Speaker Job Title` = tolower(df_4$`Speaker Job Title`)

df_5 = df_4 %>%
        group_by(df_4$`Speaker Job Title`, df_4$Label) %>%
        dplyr::summarise(n = n())

t = c("SpeakerJobTitle","Label","Count")
colnames(df_5)<- as.vector(t)

```

```{r fig.height=12}
#Do high positions of authority lead to misleading information - based on the distribution of data in dataset ?

ggplot(df_5, aes(x=df_5$SpeakerJobTitle, y=df_5$Count))+
  geom_bar(stat='identity', fill="forest green")+
  ylab("Count") +  xlab("Speaker Job Title") +
  facet_wrap(~df_5$Label,  ncol=1) +
  theme_ben2()
```


```{r fig.width=20, fig.height=20}
# Who In party (Higher or lower positions of power)  lead to corrupt information?

df_8 = df_4 %>%
        group_by(df_4$`Speaker Job Title`, df_4$`The Party Affiliation`, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n))

t = c("SpeakerJobTitle","ThePartyAffiliation", "Label","Count")
colnames(df_8)<- as.vector(t)

df_9 = filter(df_8, Label == "false" | Label == "half-true")

df_10 = filter(df_9, ThePartyAffiliation=="independent" | ThePartyAffiliation=="republican" | ThePartyAffiliation=="democrat")

```


```{r fig.height=10}
knitr::opts_chunk$set(
 fig.width = 6,
 fig.asp = 0.8,
 out.width = "80%"
)
# Stacked
ggplot(df_10, aes(fill=df_10$Label, y=df_10$Count, x=df_10$SpeakerJobTitle)) + 
  geom_bar(position="stack", stat="identity") + 
  xlab("Speaker Job Title") +
  ylab("Count") + 
  labs(fill='Label') +
  facet_wrap(~df_10$ThePartyAffiliation,  ncol=1) + theme_ben2()

```

```{r fig.width=13, fig.height=13}

# Most selected topic by each authority power, and relationship with fake news

df_6 = df_4 %>%
        group_by(df_4$`Speaker Job Title`, df_4$`Subject(s)`, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n)) %>%
        filter(n>=4)

t = c("SpeakerJobTitle","Subjects", "Label","Count")
colnames(df_6)<- as.vector(t)

df_12 = filter(df_6, Label == "false" | Label == "half-true")

ggplot(df_12, aes(fill=df_12$SpeakerJobTitle, y=df_12$Count, x=df_12$Subjects)) + 
  geom_bar(position="dodge", stat="identity") +
  xlab("Subjects") +
  ylab("Count") +
  labs(fill = "Speaker Job Title") + scale_fill_viridis(discrete=TRUE) + 
  theme_ben() + coord_polar() 


```

```{r}

#Increasing Power & medium chosen to propogate fake news

df_10 = df_4 %>%
        group_by(df_4$`Speaker Job Title`, df_4$`Venue/Location`, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n)) %>%
        filter(n>=4)

t = c("SpeakerJobTitle","Venue", "Label","Count")
colnames(df_10)<- as.vector(t)

df_11 = filter(df_10, Label == "false" | Label == "half-true")

```



```{r}
#Reference : https://dk81.github.io/dkmathstats_site/rtext-freq-words.html

df_19 = filter(df_fakeNews,Label=="false" | Label =="barely-true" | Label=="half-true" | Label=="pants-fire")

df_text <- Corpus(VectorSource(df_19$Statement))
df_text_clean <- tm_map(df_text, removePunctuation)
df_text_clean <- tm_map(df_text, content_transformer(tolower))
df_text_clean <- tm_map(df_text, removeNumbers)
df_text_clean <- tm_map(df_text, stripWhitespace)
df_text_clean <- tm_map(df_text, removeWords, stopwords('english'))

wordcloud(df_text_clean, scale = c(2, 1), min.freq = 100, colors = rainbow(30))
```
```{r}
df_20 = filter(df_fakeNews,Label=="true" | Label =="mostly-true" )

df_text <- Corpus(VectorSource(df_20$Statement))
df_text_clean <- tm_map(df_text, removePunctuation)
df_text_clean <- tm_map(df_text, content_transformer(tolower))
df_text_clean <- tm_map(df_text, removeNumbers)
df_text_clean <- tm_map(df_text, stripWhitespace)
df_text_clean <- tm_map(df_text, removeWords, stopwords('english'))

wordcloud(df_text_clean, scale = c(2, 1), min.freq = 100, colors = rainbow(30))

```


```{r fig.width=13}
#reference :-https://dk81.github.io/dkmathstats_site/rtext-freq-words.html

df_text <- data_frame(Text = df_19$Statement)
df_text_word <- df_text %>% 
                unnest_tokens(output = word, input = Text)
df_text_word <- df_text_word %>%
                   anti_join(stop_words) # Remove stop words in peter_words

df_17 = df_text_word %>%
        group_by(df_text_word$word) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n)) 

df_17$`df_text_word$word` <- gsub('[0-9.]', '', df_17$`df_text_word$word`)
df_18 = filter(df_17,df_17$`df_text_word$word`!="")

t = c("word","n")
colnames(df_18)<- as.vector(t)


df_18 %>% 
  filter(n > 500) %>%
   mutate(word = reorder(word, n)) %>% 
    ggplot(aes(word, n)) + 
    geom_col() +
    coord_flip() +
    labs(x = "Word \n", y = "\n Count ", title = "Frequent Words In Peter Pan \n") +
    geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") + theme_ben()

```


```{r}
#Relation of number of punctuation with each Speaker and the label assigned

#References : https://rdrr.io/github/Amherst-Statistics/katherinemansfieldr/src/R/extract.R

extract_punct <- function(text){
  output <- as.vector(tolower(text))
  output <- strsplit(output, "( [a-z]+|[a-z]+)") %>%
    unlist()
  output <- output[which(output != "")]
  output <- strsplit(output, " ") %>%
    unlist()
  output <- gsub("\u201D", "~\u201D", output)
  output <- strsplit(output, "~", perl = TRUE) %>%
    unlist()
  return(output)
}

countPunctuations <- vector()
LabelData <- vector()
SpeakerJobTitle <- vector()

for(i in 1:nrow(df_4)) {
  countPunctuations[i] = length(extract_punct(df_4$Statement[i]))
  LabelData[i] = df_4$Label[i]
  SpeakerJobTitle[i] = df_4$`Speaker Job Title`[i]
}

df_14 = data.frame(countPunctuations,LabelData,SpeakerJobTitle)

df_15 = aggregate(df_14$countPunctuations, by=list(df_14$SpeakerJobTitle,df_14$LabelData), FUN=sum)

df_16 = filter(df_15, x<=1000)

ggplot(df_16, aes(x = x, y = reorder(Group.1, x))) +
  geom_point(aes(colour = Group.2)) +   # Use a larger dot 
  xlab("Count of Punctuations in Statement") + ylab("Speaker Job Title")
```
  

```{r fig.width=12}
# What are the states with maximum proportions of fake news and in what subject? Which subject has the most concentrated fake news?


# Which state makes the most claims?
# Reference for ordering: https://www.statmethods.net/management/sorting.html

filt = c("Virginia director, Coalition to Stop Gun Violence", "Unknown", "unknown", "N/A", "Tex", "Russia", "Qatar", "China", "Virgiia", "PA - Pennsylvania", "None", "United Kingdom", "the United States")
df_f3 <- df_fakeNews %>%
  filter(!(df_fakeNews$`State Info` %in% filt))

df_f4 = df_f3 %>%
        group_by(df_f3$`State Info`, df_f3$`Label`) %>%
        dplyr::summarise(n = n())
t = c("StateInfo","Label","Count")
colnames(df_f4)<- as.vector(t)


df_order <- df_f3 %>%
        group_by(df_f3$`State Info`) %>%
        dplyr::summarise(n = n())
colnames(df_order)<- as.vector(c("StateInfo", "n"))
df_order <- df_order[order(-df_order$n),]
df_order$Panel <- c(rep(1:6, each = 9), c(6,6,6))
df_order$Panel <- factor(df_order$Panel, levels = 1:6,
                   labels = c("250+ Tweets", "80-270 tweets", "30-80 tweets",
                              "6-30 tweets", "2-6 tweets", "1 or 2 tweets"))

df_f4$total <- df_order$n[match(df_f4$StateInfo, df_order$StateInfo)]
df_f4$panel <- df_order$Panel[match(df_f4$StateInfo, df_order$StateInfo)]
df_f4 <- df_f4[order(-df_f4$total),]

lvl = c("pants-fire", "false", "barely-true", "half-true", "mostly-true", "true")
df_f4 %>%
ggplot(aes(fill = factor(Label, levels = lvl), y = Count, x = reorder(StateInfo,total) )) + 
  geom_bar(position="stack", stat="identity") + 
  facet_wrap(~panel, ncol=1, scales="free") +
  xlab("State Info") +
  ylab("Count") + 
  labs(fill='Label') + 
  coord_flip() +
  theme_ben2(ang=0, add=2)

```




```{r fig.width=13}
# Reference: https://stackoverflow.com/questions/32578082/r-how-to-replace-value-of-a-variable-conditionally 


# Getting the percentage of lies for states with over 10 statements
dff <- df_f3
df_ord <- dff %>%
        group_by(dff$`State Info`) %>%
        dplyr::summarise(n = n())
colnames(df_ord)<- as.vector(c("StateInfo", "n"))

false = c("barely-true", "false", "pants-fire", "half-true")
true = c("mostly-true", "true")

dff$Label <- with(dff, ifelse(Label %in% false, "not factual", Label))
dff$Label <- with(dff, ifelse(Label %in% true, "factual", Label))
encoded <- dff



# Reference: https://statisticsglobe.com/select-top-n-highest-values-by-group-in-r 
txt <- data_frame(Text = encoded$`Subject(s)`, Label=encoded$`Label`, State=encoded$`State Info`) %>%
  unnest_tokens(output = word, input = Text, token = 'regex', pattern=",")
txt_total <- txt %>% group_by(txt$word, txt$State) %>% 
  dplyr::summarise(n = n()) %>% 
  arrange(desc(n))
txt <- txt %>%
  group_by(txt$word, txt$Label, txt$State) %>% 
  dplyr::summarise(n= n()) %>% 
  arrange(desc(n))
t1 = c("word", "Label", "state", "n")
t2 = c("word", "state", "total")
colnames(txt) <- as.vector(t1)
colnames(txt_total) <- as.vector(t2)
txt <- merge(txt, txt_total)


prop <- txt %>%
  filter(Label == "not factual") %>%
  filter(total > 2)
prop$perc <- round(100*prop$n/prop$total, digits=1)
prop <- prop %>% 
  arrange(desc(perc)) %>% 
  group_by(state) %>%
  slice(1:1)


dff = dff %>%
        group_by(dff$`State Info`, dff$`Label`) %>%
        dplyr::summarise(n = n())
colnames(dff) <- as.vector( c("StateInfo","Label","Count") )
dff$total <- df_ord$n[match(dff$StateInfo, df_ord$StateInfo)]
dff <- dff %>%
        filter(Label == "not factual") %>%
        filter(total > 10)
dff$liesPercentage <- round(100*dff$Count/dff$total, digits=1)
dff$word = prop$word[match(dff$StateInfo, prop$state)]


# Reference: https://stackoverflow.com/questions/45437815/geom-text-with-two-labels-in-ggplot-graph

dff %>%
  ggplot(aes(y = liesPercentage, x = reorder(StateInfo, liesPercentage))) +
  geom_col() +
  coord_flip() +
  labs(x = "State \n", y = "\n Percentage of Lies", title = "Percentage of lies by State\n") +
  geom_text(aes(label = paste(liesPercentage,"%, ",word)), hjust = 1.2, colour = "white", fontface = "bold") +
  theme_ben(ang=0)
```


```{r}
dfl <- df_f3
dfl$Label <- with(dfl, ifelse(Label=="pants-fire", 0, Label))
dfl$Label <- with(dfl, ifelse(Label=="false", 1, Label))
dfl$Label <- with(dfl, ifelse(Label=="barely-true", 2, Label))
dfl$Label <- with(dfl, ifelse(Label=="half-true", 3, Label))
dfl$Label <- with(dfl, ifelse(Label=="mostly-true", 4, Label))
dfl$Label <- with(dfl, ifelse(Label=="true", 5, Label))
dfl <- transform(dfl, Label = as.numeric(Label))
```

```{r fig.width=12}
ggplot(dfl, aes(x = Label, y = reorder(State.Info, -Label, median))) + 
  geom_density_ridges(fill = "blue", alpha = .5) +
  labs(x = "Score", y = "State", title = "Distribution of Misinformation by State") +
  theme_ben2(ang=0)
```


```{r fig.width=13}
txt <- data_frame(Text = encoded$`Subject(s)`, Label=encoded$`Label`) %>%
  unnest_tokens(output = word, input = Text, token = 'regex', pattern=",")
txt_total <- txt %>% group_by(txt$word) %>% 
  dplyr::summarise(n = n()) %>% 
  arrange(desc(n))
txt <- txt %>%
  group_by(txt$word, txt$Label) %>% 
  dplyr::summarise(n= n()) %>% 
  arrange(desc(n))
t1 = c("word", "Label", "n")
t2 = c("word", "n")
colnames(txt) <- as.vector(t1)
colnames(txt_total) <- as.vector(t2)
txt$total <- txt_total$n[match(txt$word, txt_total$word)]
txt <- txt %>% 
  arrange(desc(total))


prop <- txt %>%
  filter(Label == "not factual") %>%
  filter(n > 50)

prop$liesPercentage <- round(100*prop$n/prop$total, digits=1)
prop <- prop %>%
  arrange(-liesPercentage) %>%
  head(50)
prop$Panel <- rep(1:2, each = 25)
prop$Panel <- factor(prop$Panel, levels = 1:2,
                   labels = c("Top 25 Controversial Subjects", "Top 25-50"))

prop %>%
  ggplot(aes(y = liesPercentage, x = reorder(word, liesPercentage))) +
  geom_col() +
  coord_flip() +
  labs(x = "Subject \n", y = "\n Percentage Fake News", title = "Percentage of lies by Subject\n") +
  geom_text(aes(label = liesPercentage), hjust = 1.2, colour = "white", fontface = "bold") +
  facet_wrap(~Panel, ncol=1, scales="free") + 
  theme_ben(ang=0)

```


