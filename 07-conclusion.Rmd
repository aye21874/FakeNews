# Conclusion

<!-- Summary

<!-- 1) For this section, we have tried answering 10 questions below, which we find  intriguing from the dataset.  -->

<!-- 2) For each of them we tried summarizing the steps on how we approached this particular question and what assumptions were taken.  -->

<!-- 3) We have also summarized each graph and in the process answered each relevalant question.  -->

<!-- Note :- We have used the data from "FakeNews_Clean.csv" obtained after cleaning/processing the datasset. From this file, we have created 2 dataframes. One for the Fake News grouped by Labels ("false","half-true","pants-fire","barely-true") and the other by Factual News grouped by Label ("True", "stly-True").  -->


```{r show_col_types = FALSE, warning=FALSE, error=FALSE}
library(tidyverse)
library(patchwork)
library(ggplot2)
library(mi)
library(extracat)
library(dplyr)
library(plyr)
library(vcd)
library(grid)
library(tidyverse)
library(gridExtra)
library(scales)
library(openintro)
library(GGally)
library(ggplot2)
library(parcoords)
library(Lock5withR)
library(htmltools)
library(d3r)
library(ggalluvial)
library(wordcloud)
library(tm)
library(tidytext)
library(ggridges)
library(geofacet)
library(viridis)
library(usmap) #import the package
```


```{r}
theme_fakeNews_rotate <- function(base_size = 14, ang=90) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 25, face = "bold", margin = margin(10,10,10,10), hjust = 0.5),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.x = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.y = element_text(size = 13, face="bold", margin = margin(10,10,10,10)),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 13, face = "bold"),
      legend.text = element_text(size = 13, face = "bold")
    )}

theme_fakeNews <- function(base_size = 14) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = 25, face = "bold", margin = margin(10,10,10,10), hjust = 0.5),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.x = element_text(size = 13, face = "bold", margin = margin(10,10,10,10)),
      axis.text.y = element_text(size = 13, face="bold", margin = margin(10,10,10,10)),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = 13, face = "bold"),
      legend.text = element_text(size = 13, face = "bold")
    )}
```



```{r}

# Reading from the file
# Filtering based on the Label. 
# Created 2 dataframes, 
    # One for fake news (grouped by Label["false","half-true","pants-fire","barely-true"]) 
    # and other one for Factual News (grouped by Label["True", "Mostly-True"])

fakeNews_df_transformed = read_csv(file = 'FakeNews_Clean.csv' )
fakeNews_df_transformed_false = filter(fakeNews_df_transformed, Label=="false"| Label =="barely-true" | Label=="half-true" | Label=="pants-fire")
fakeNews_df_transformed_true = filter(fakeNews_df_transformed, Label=="true"| Label =="mostly-true") 
```


**Question 1 : Which positions of authority lead to Fake & Factual information? Which Party tends to spread Fake News More? **

```{r fig.height=12, fig.width=20}
df_1 = fakeNews_df_transformed %>% 
        group_by(fakeNews_df_transformed$SpeakerJobTitle) %>% 
        dplyr::summarise(n = n()) %>%
        arrange(desc(n)) %>%
        filter(n>=100) 


t = c("SpeakerJobTitle","Count")
colnames(df_1)<- as.vector(t)

new_vector <- df_1$SpeakerJobTitle


df_3 <- fakeNews_df_transformed[fakeNews_df_transformed$SpeakerJobTitle %in% new_vector,]

df_4 <- df_3 %>%
        filter(df_3$SpeakerJobTitle != "unknown")


#VVIMPT
df_4$SpeakerJobTitle = tolower(df_4$SpeakerJobTitle)


df_5 = df_4 %>%
        group_by(df_4$SpeakerJobTitle, df_4$Label) %>%
        dplyr::summarise(n = n())


t = c("SpeakerJobTitle","Label","Count")
colnames(df_5)<- as.vector(t)

df_5$SpeakerJobTitle=  df_5$SpeakerJobTitle %>% recode(
                      "attorney" = "Attorney",
                      "congress" = "Congress",
                      "president" = "President",
                      "presidentelect" = "President Elect",
                      "governor" = "Governor",
                      "presidential candidate" = "President Candidate",
                      "house representative, house representatives" = "House_Official",
                      "milwaukee county executive" = "County Executive",
                      "senator, senators" = "Sentators",
                      "congress, congressman, congressman ny, congresswoman" = "Congress",
                      "representative, representativej" = "Representative")
                      
          
#Do high positions of authority lead to misleading information - based on the distribution of data in dataset ?

ggplot(df_5, aes(x=df_5$SpeakerJobTitle, y=df_5$Count))+
  geom_bar(stat='identity', fill="forest green")+
  ylab("Count for Fake & Factual News") +  xlab("Speaker Job Title") +
  facet_wrap(~df_5$Label,  ncol=1) +
  theme_fakeNews_rotate()
```

**Observation from Graph 1**

1) President, Governor & Senators from different Party are most popular "Speaker Job Titles" in spreading both Fake & Factual News. 

2) The "President Elect" & "Predient Candidate" had almost equal counts for spreading Fake or Factual News

3) Senators from the Party lead the count for spreading information, for both Factual & Fake. The graph also demonstrates highest Pants-Fire count for Senators, deonting that they are the one's with most latest information related to Party. 

4) For any "Speaker Job Title", the counts for mostly-true labels were higher than the true.

5) House Officials have the least power in spreading Fake or Factual News. 

6) The overall projects in counts for False and Barely True lead the other labels, signifying that almost all Speaker Job Titles from Graph spread high propotion of Fake News. 



```{r fig.height=12, fig.width=20}

df_8 = df_4 %>%
        group_by(df_4$SpeakerJobTitle, df_4$PartyAffiliation, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n))

t = c("SpeakerJobTitle","ThePartyAffiliation", "Label","Count")
colnames(df_8)<- as.vector(t)

df_9 = filter(df_8, Label == "false" | Label == "half-true")

df_10 = filter(df_9, ThePartyAffiliation=="independent" | ThePartyAffiliation=="republican" | ThePartyAffiliation=="democrat")

df_10$SpeakerJobTitle=  df_10$SpeakerJobTitle %>% recode(
                      "attorney" = "Attorney",
                      "congress" = "Congress",
                      "president" = "President",
                      "presidentelect" = "President Elect",
                      "governor" = "Governor",
                      "presidential candidate" = "President Candidate",
                      "house representative, house representatives" = "House_Official",
                      "milwaukee county executive" = "County Executive",
                      "senator, senators" = "Sentators",
                      "congress, congressman, congressman ny, congresswoman" = "Congress",
                      "representative, representativej" = "Representative")


knitr::opts_chunk$set(
 fig.width = 6,
 fig.asp = 0.8,
 out.width = "80%"
)
# Stacked
ggplot(df_10, aes(fill=Label, y=Count, x=SpeakerJobTitle)) + 
  geom_bar(position="stack", stat="identity") + 
  xlab("Speaker Job Title") +
  ylab("Count for Fake News") + 
  labs(fill='Label') +
  facet_wrap(~ThePartyAffiliation,  ncol=1) + theme_fakeNews()

```

**Observation from Graph 2**

1) Republican Party is more prone towards  spreading Fake News. 
2) The Propotion of Half-True is more than False for almost all Speakers belonging to any party. This tells us that significane of some truth in any given statement is high.


**Question 2: What are the most selected topic/Subject picked by each Speaker which has the highest tendency of Spreading Fake News"**

```{r fig.width=17, fig.height=10}

# Most selected topic by each authority power, and relationship with fake news

df_6 = df_4 %>%
        group_by(df_4$SpeakerJobTitle, df_4$Subject, df_4$Label) %>%
        dplyr::summarise(n= n()) %>%
        arrange(desc(n)) %>%
        filter(n>=4)

t = c("SpeakerJobTitle","Subjects", "Label","Count")
colnames(df_6)<- as.vector(t)

df_12 = filter(df_6, Label == "false" | Label == "half-true")


df_12$Subjects=  df_12$Subjects %>% recode(
                     "education"="Education",
                     "healthcare"="HealthCare",
                     "elections"="Elections",
                     "immigration"="Immigrations",
                     "economyjobs"="EconomyJobs",
                     "federalbudget"="Fedral Budget",
                     "labor, taxes, water, weather, women" = "Labor, Weather, Taxes",
                     "abortion"="Abortion",
                     "debt, food, guns, jobs, polls" = "Debts, Food, Jobs",
                     "fires, iraq, islam, israel, trade, urban" = "Fires, Iraq, Israel, Islam",
                     "energy"="Energy",
                     "candidatesbiography"="Candidates Biography",
                     "economy"="Economy",
                     "foreignpolicy"= "Foreign Policy",
                     "military" = "Military")

df_12$SpeakerJobTitle = df_12$SpeakerJobTitle %>% recode(
                      "attorney" = "Attorney",
                      "congress" = "Congress",
                      "president" = "President",
                      "presidentelect" = "President Elect",
                      "governor" = "Governor",
                      "presidential candidate" = "President Candidate",
                      "house representative, house representatives" = "House_Official",
                      "milwaukee county executive" = "County Executive",
                      "senator, senators" = "Sentators",
                      "congress, congressman, congressman ny, congresswoman" = "Congress",
                      "representative, representativej" = "Representative")


ggplot(df_12, aes(fill=df_12$SpeakerJobTitle, y=df_12$Count, x=df_12$Subjects)) + 
  geom_bar(position="dodge", stat="identity") +
  xlab("X Asix") +
  ylab("Y Axis") +
  labs(fill = "Speaker Job Title") + scale_fill_viridis(discrete=TRUE) + 
  theme_fakeNews() + coord_polar() 


```


**Observation from Graph**

1) Senators have spread Fake News in highest propotion with respect to every topic demonstrated in graph

2) President Candidate had Fake News only related to Immigration

3) President didn't had Fake News related to topics like Military, Abortion, Debts, Jobs, Economy, 

4) Only Senators and President had fake news related to topics :- Fires, Iraq, Israel, Islam, Foreign Policy, Fedral Budge, Energy. 

5) Education Subject had the highest count for fake news and the most selected topic by any speaker. 

2) The Propotion of Half-True is more than False for almost all Speakers belonging to any party. This tells us that significane of some truth in any given statement is high.


**Question 3:- States Spreading maximum number of false counts, noting the count of different speakers involved in spreading Fake News**

```{r fig.width=13, fig.height=8}

df_80 = fakeNews_df_transformed_false %>% 
      group_by(fakeNews_df_transformed_false$StateInfo) %>% 
      dplyr::summarise(n = n()) %>%
      arrange(desc(n)) 

t = c("State","NumberofFalseCounts")
colnames(df_80)<- as.vector(t)

df_81 = fakeNews_df_transformed_false %>%
        group_by(fakeNews_df_transformed_false$StateInfo, fakeNews_df_transformed_false$Speaker) %>% 
        dplyr::summarise(n = n()) %>%
        arrange(desc(n)) 

t = c("State"," UniqueSpeakerPerState","Count")
colnames(df_81)<- as.vector(t)

df_82 = df_81 %>% 
      group_by(df_81$State) %>% 
      dplyr::summarise(n = n()) %>%
      arrange(desc(n)) 


t = c("State","NumberofSpeakers")
colnames(df_82)<- as.vector(t)

df_80["NumberofSpeakers"] =df_82$`NumberofSpeakers`
df_83 = df_80 %>% filter(NumberofFalseCounts>20)
```


```{r fig.width=13, fig.height=10, echo=FALSE, message=FALSE, results='hide'}

treemap::treemap(df_83,
       index=c("State"),
       vSize="NumberofFalseCounts",
       vColor="NumberofSpeakers",
       type="value",
       format.legend = list(scientific = FALSE, big.mark = " ")) + theme_fakeNews()
```

**Observation from the Graphs**

Above Graph gives a clear idea for the count of Fake News in each State. The larger the rectangle, the state had more number of Fake News Spread. The color is mapped with the different number of speakers involved in that state to spread Fake News. 

1) There were lot of states in the dataset which were labelled Unknown while pre-processing, which apparently are also spreading Fake News in high propotions

2) Texas is most widely known state to spread Fake News with about 400+ different Speakers involved in spreading Fake News

3) Count of Fake News Spread is closely related with the different number of Speakers spreading the news. North Carolina, Tennessee, Colorado, Alaska etc, which share the same size, have least number of different speakers observed.

4)  New York, Wisconsin and Florida have almost equal number of counts in spreading Fake News, which are compartively high when compare to other states. Also, Wisconsin and New York share the same count for number of speakers but the Florida has most number of different speakers among the 3, spreading the Fake News. 




**Question 4 :- The Most Prone Fake News Method of communication, State Wise **

```{r fig.width=25 , fig.height=10}

# We have used Catograms to answer this question. Below is the step wise procedure for the same. 
# 
# Step 1) Columns Used Label, Venue, State. 
# 
# Step 2) Generate the count for Fake News spread using a particular Platform (Venue) in a particular region (StateInfo). 
# 
# Step 3) Filtered the Unknown States from the dataset, by removing those rows. An important step here will be to make sure regions in both the dataframes are either lower case or upper case so that they can be matched.
# 
# Step 4) For each state, obtained the Platform (Venue) with max fake news count. We also avoided the Platforms which had a count of 1 Fake News, which was not enough to validate if Platform is actually spreading fake news.
# 
# Step 5)  Using the state dataframe of USA (imported form library), merged our dataframe (using left join), to get longitudes and latitudes of each states, so that they can plotted on the Catogram.
# 
# Step 6) To make sure that Venues like "news conference, press conference", which were grouped together, are recoded with proper names, to make sure they have better names on the graph.


df_51 = fakeNews_df_transformed_false %>% 
        group_by(fakeNews_df_transformed_false$Venue, fakeNews_df_transformed_false$StateInfo) %>% 
        dplyr::summarise(n = n()) %>%
        arrange(desc(n)) 

t = c("GroupedVenue","region","n")
colnames(df_51)<- as.vector(t)

df_52 = filter(df_51, region != "unknown") 

state <- map_data("state")
state$region = tolower(state$region)
df_52$region = tolower(df_52$region)

df_53 = df_52 %>%
            group_by(region) %>%
            filter(n==max(n) & n>=2)

df_54 <- left_join(state, df_53, by = c('region'))

df_54$GroupedVenue=  df_54$GroupedVenue %>% recode(
                      "news conference, press conference" = "Conferences",
                      "television interview" = "Television Interview",
                      "campaign ad, campaign tv ad, campaign web ad" = "Campaign Advertisement",
                      "speech, speeches" = "Speeches",
                      "tv ad, tv ads, tv talk, web ad"="Television Advertisement",
                      "unknown" = "Unknown",
                      "internet, interview, interviews"="Online Interviews",
                      "vice presidential debate, vicepresidential debate" = "Vice-Presidential Debate",
                      "campaign commercial, campaign commerical, campaign tv commercial, tv campaign commercial" = "Commercial Campaign",
                      "senate floor speech" = "Senate Floor Speech",
                      "alert, retweet, ticket, tweet, tweets" = "Tweets",
                      "interview cnn, interview cspan, interview ed, interview union" = "Interview Cnn",
                      "news release"  = "News Release",
                      "opinion colulmn, opinion column" = "Opinion Column",
                      "comments abcs week" = "Comments Week",
                      "date, debate, senate, tv debate"  = "Debate",
                      "public statement, public statements" = "Public Statements",
                      "speech senate floor" = "Senate Speech",
                      "interview fox news sunday" = "Fox News Interview",
                      "press releaase, press release, press releases" = "Press Release",
                      )


ggplot(data=df_54, aes(x=long, y=lat, fill=GroupedVenue, group=region)) + 
  geom_polygon(color = "white") + ggtitle("Most Popular Platforms to Spread Fake News State Wise") + xlab("Longitude") + ylab("latitude") +
  theme_fakeNews() 

```


**Observation from Graph**

Below are the few names of venues (platforms) state wise which were used maximum times to spread fake news 

1) Florida, Texas, California, Indiana, Maryland, Missouri, North Carolina mostly uses Press Release as the mediumn for fake news. 

2) Georgia, Illinois, Pennsylvania, Virginia mostly uses Speeches as the medium for fake news. 

3) Since the states map imported from library had more states than the states present in our dataframe, we obderved NA in the graph. We didn't had the information related to "most widely platforms used" to spreak fake news.

4) Our Grouping of Venue Like Tweets/Tweet to Tweet might not be that accurate as we have used group_str to do the concatenation with the distance of similarity as 3 between words. This parameter can be manipualated for better efficiency. 



**Question 5:- Correlation, Clusters & Outliers observed for different Mediums/Platforms in spreading Fake News for each State **


```{r fig.width=17, fig.height=10 }

# Step 1) Columns Used are :- Label, Venue (Platforms)
# 
# Step 2) Using the fake news dataframe, generate the count for Fake News spread using a particular Platform (Venue) 
# 
# Step 3) Pivot Wider the dataset to create columns for each unique value present in the Venue. Now, we will have a dataframe with each Region covering the count for the times the particular Platform/Venue was used to spread Fake News.
# 
# Step 4) Since, not all platforms/Venue were not used for each state, we selected a particular portion of dataframe (13 states and 10 Platforms/Venue) which had the least null values. 
# 
# Step 5) Then we imputed the null values with the mean of each row i.e. mean of platforms used for that region
# 
# Step 6) Recoded the column names for better understanding in graph and plotted them using the ggparcoord 


df_60 = df_52 %>%
            group_by(GroupedVenue)

df_61 = df_60 %>%
  pivot_wider(names_from = GroupedVenue, values_from = n)

#snapshot with almost no missing values
df_62 = df_61[1:13, 1:10]

df_63 = df_62 %>% replace(is.na(.), 0) %>% mutate(row_wise_mean = rowMeans(.[2:10])) 

for (row in 1:nrow(df_63)) {
  for(col in 1:(ncol(df_63)-1)){
    if(df_63[row,col]==0)
    df_63[row,col] = as.integer(df_63[row,"row_wise_mean"])
  }
}
t = c("Region","Online Interview","Press Release","News Release","TV Advertisement","Speeches","News Conferences","Radio Interview","Tweets", "TV Interview","Row_Wise_Mean")
colnames(df_63)<- as.vector(t)

ggparcoord(df_63, columns =2:10, scale = "globalminmax", groupColumn = 1)  + theme_fakeNews() + xlab("Platform Used for fake news") + ylab("Count for total number of times platform was used to spread Fake News")


# df_63[,1:10]  %>% 
#   parcoords(
#     rownames = F
#     , brushMode = "1D-axes"
#     , reorderable = T
#     , queue = T
#     , color = list(
#     colorScale = "scaleOrdinal",
#     colorBy = "Region",
#     colorScheme = "schemeCategory10"
#   ),
#   ,withD3 = TRUE
#   ,height = 700
#   ,width = 1000
#   )

```


**Observation from the Graphs**

1) The count for OnlineInterview Medium for spreading Fake News was mostly less (<10) for most of the regions, except Wisconscin, Texas and Florida.

2) Most of the States, who have used Press Release for spreading Fake News, have also used News Release. We observed Positive Correlation between these two mediums for almost all the regions

3) Similarly, we noticed negative correlation between News Release and TV Advertisement. Those states which have used News Release have a reduce count for the TV Advertisement

4) We noticed wide range of differences in count between New York & Jersy for each above mentioned platform in graohw which suggest different political administration. 

5) Even Tweeter, the most popular online social platform waas not used as the medium to spread Fake News for these states. 



**Question 6: Count of  Words in Statment and its relationship with Label for the News**


```{r fig.width=12, fig.height=10}

wordsCount <- vector()
LabelData <- vector()
SpeakerJobTitle <- vector()


for(i in 1:nrow(fakeNews_df_transformed)) {
  split <- strsplit(fakeNews_df_transformed$Statement[i], " ")
  t = sapply( split , length)
  wordsCount[i] = t
  LabelData[i] = fakeNews_df_transformed$Label[i]
  SpeakerJobTitle[i] = fakeNews_df_transformed$SpeakerJobTitle[i]
}



df_21 = data.frame(wordsCount,LabelData,SpeakerJobTitle)
df_22 = aggregate(df_21$wordsCount, by=list(df_21$SpeakerJobTitle,df_21$LabelData), FUN=sum)
df_23 = filter(df_22, x>=100 & x<=1000)

df_24 = df_23 %>% 
      mutate(newLabel = ifelse((Group.2=="false" | Group.2 =="barely-true" | Group.2=="half-true" | Group.2=="pants-fire"), "false","true"))


df_24$Group.1=  df_24$Group.1 %>% recode(
                "actor, auditor, author, mother" = "Actor/Author",
                "advocacy" = "Advocacy",
                "attorney"  = "Attorney",
                "banker, farmer, laws, lawyer, mayor" = "Banker/Lawyer",
                "businessman, businesswoman"  = "Business Individual",
                "candidate senate physician" = "Physician Senate",
                "cohost cnns crossfire"  = "Cohost Cnns",
                "congress, congressman, congressman ny, congresswoman" = "Congress Official",
                "consultant" = "Consultant",
                "governor jersey" = "Governor Jersey", 
                "host, house" = "House Host",
                "house district" = "House District",
                "house majority leader, house minority leader" = "House Majority Leader",
                "house representative, house representatives" = "House Official",
                "lieutenant governor"  = "Lieutenant_Governor",
                "mayor austin, mayor houston"= "Mayor",
                "milwaukee county executive" = "County Executive",
                "ohio governor" = "Ohio Governor",
                "political action committee, politican action committee" = "Action Committee",
                "presidentelect" = "President Elect",
                "presidential candidate" = "Presidential Candidate",
                "radio host, radiotv host" = "Radio Host",
                "representative florida district" = "Florida Official",
                "senate majority leader, senate minority leader" = "Senate Majority",
                "senator district, senator st district"  = "Senator District",
                "senator ohio" = "Senator Ohio",
                "social media posting" = "Social Media Posting" ,
                "speaker house representatives, speaker nc house representatives" = "Speaker House",
                "texas congressman houst representatives" = "Texas Congress",
                "chairman republican national committee, cochairman republican national committee" = "Chairman committee",
                "columnist" = "Columnist",
                "governor ohio jan" = "Governor Ohio",
                "mayor milwaukee" = "Mayor Milwaukee",
                "mayor providence" = "Mayor Providence",
                "ohio treasurer, ri treasurer, treasurer" = "Treasurer",
                "political commentator" = "Political Commentator",
                "venture capital company founder" = "Vebture Capital",
                "assembly district"  = "Assembly District",
                "assemblyman, assemblywoman"  = "AssemblyMan/Woman",
                "delegate" = "Delegate",
                "madison school board" = "Madison School",
                "philanthropist" = "Philanthropist",
                "lieutenant governorelect" = "Lieutenant Elect Governor",
                "president heritage foundation" = "President Heritage",
                "secretary"  = "Secretary",
                "president" = "President",
                "representative, representativej" = "Party Representative ",
                "agriculture commissioner" = "Agriculture Commissioner",
                 "msnbc host" = "Msnbc Host",
                 "texas house representative" = "Texas House Official")
                                                                 

t = c("SpeakerJobTitle","Label","Count","News_Label")
colnames(df_24)<- as.vector(t)


ggplot(df_24, aes(x = Count, y = reorder(SpeakerJobTitle, Count))) +
  geom_point(aes(colour = News_Label)) +   
  xlab("Count of Words in Statement") + ylab("Speaker Job Title") + theme_fakeNews()

```

**Observation from the Graphs**

For Statements between 100 and 1000 Words,

1) The propotion for the True Labels are high with the word counts less than 300 in the statements made by any speaker

2) All Speakers had False Count in more propotion over True Count.



